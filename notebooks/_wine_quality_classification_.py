# -*- coding: utf-8 -*-
""""Wine Quality Classification"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13ethYwqRkoOKUGMvtcEDCPdcjQNnJeE2

Мы провели исследование на наборе данных Kaggle.com Wine Quality, который изначально взят из репозитория машинного обучения UCI. Набор данных содержит 6497 строк.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import seaborn as sns
import pandas as pd 
import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.metrics import roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE


from sklearn.model_selection import cross_validate, GridSearchCV, train_test_split, StratifiedKFold


import warnings
warnings.filterwarnings("ignore")
# %matplotlib inline

"""# DOWNLOAD DATASET"""

# скачаем датафрейм
data = pd.read_csv('winequalityN.csv')

data.head()

"""Набор данных был импортирован без ошибок с соответствующими заголовками столбцов. Дополнительных работ не требуется."""

# проверим распределение классов
data.quality.value_counts()

"""Вино состоит из многих элементов, которые способствуют его качеству. Стоит разобраться, что означают эти черты в вине. Вот их небольшой обзор:
- Тип

Цвет вина зависит от цвета винограда: красное вино производится из красного и черного винограда, а белое вино — из белого винограда. При производстве белого вина семена и кожица удаляются, в то время как при производстве красного вина этого не требуется. Эта разница в процессе влияет на вкус и цвет, делая красное вино более насыщенным, а белое вино более легким и фруктовым. Этот набор данных имеет атрибут type, который идентифицирует цвет вина.

- Кислотность

Кислотность в вине очень важна, так как она влияет на вкус. Вино с высокой кислотностью можно определить как бодрящее, яркое, пикантное, угловатое, живое, хрустящее, свежее, крепкое. Идеально сочетается с едой. В то время как пониженная кислотность будет охарактеризована как жирная, несвежая, мягкая. Кислотность – важнейшая характеристика игристых вин. Помимо изменения вкуса вина, он также позволяет ему дольше храниться, поскольку выступает в качестве естественного консерванта структуры вина. Кислоты бывают разные:

Фиксированная кислотность: происходит от кислот, содержащихся в винограде. Фиксированная кислотность в наборе данных выражена в г(винная кислота)/дм3.
Летучая кислотность: вызвана присутствием уксусной кислоты. Если присутствует небольшое количество летучих кислот, это усиливает запах вина. Когда летучие кислоты слишком высоки, они добавляют уксусный привкус и ухудшают качество вина. Летучая кислотность выражена в г(уксусной кислоты)/дм3 в наборе данных.
Лимонная кислота: вводится в качестве добавки во время процесса брожения вина и помогает повысить общую кислотность. Он также используется для целей стабилизации. Лимонная кислота в наборе данных выражена в г/дм3. В винном мире pH (потенциал водорода) — это числовая шкала, определяющая кислотность вина. Все вина находятся на кислой стороне спектра pH, и большинство из них колеблется от 2,5 до примерно 4,5 pH (напиток с pH = 7 нейтрален, вода).

- Остаточный сахар

Сладость вина исходит от сахаров в виноградном соке: глюкозы и фруктозы. Различные уровни этих элементов способствуют образованию остаточного сахара. Другие источники сахара также могут быть добавлены в процессе ферментации, например, кукурузный сахар, дрожжи. Остаточный сахар является мерой количества сухих веществ сахара в конце процесса производства вина. Остаточный сахар в наборе данных выражается в г/дм3.

- Хлориды

Хлориды – это количество соли в вине. Это происходит из-за различных аспектов винограда и условий окружающей среды, в которых он вырос. Хлориды в наборе данных выражены в г(хлорид натрия)/дм3.

- Сульфаты

Сульфаты в вине появляются в процессе его брожения. Сульфаты в вине обратно пропорциональны кислотности и цвету. Остаточный сахар прямо пропорционален количеству сульфитов для предотвращения брожения. Сульфаты встречаются в природе, но их также можно добавлять в процессе. Они помогают защитить вино от нежелательных микробов и окисления. Существует несколько способов измерения сульфитов в вине: свободный диоксид серы, общий диоксид серы и сульфаты.

Свободный диоксид серы: чрезмерное количество свободного диоксида серы может быть заметно для потребителей, скрывая ароматы вина и препятствуя его насыщению кислородом в процессе дыхания. В высоких концентрациях придает резкий, горьковатый, металлический привкус. Диоксид серы в основном используется для уничтожения вредных бактерий, но в то же время для сохранения качества и свежести. Свободный диоксид серы в наборе данных выражен в мг/дм3.

Общий диоксид серы: добавляется для уничтожения бактерий и сохранения качества вина. Чрезмерное количество этого диоксида может повредить вино, добавив нежелательный запах. Общий диоксид серы в наборе данных выражен в мг/дм3.

Сульфаты: минеральные соли, содержащие серу. Сульфаты необходимы для производства вина. Он способствует аромату и вкусу вина. Сульфаты выражены в г (сульфат калия)/дм3 в наборе данных.

- Алкоголь

Спирт является результатом преобразования сахара дрожжами в этанол в процессе ферментации. Уровень алкоголя влияет на вкус вина, более высокий уровень алкоголя делает вкус более сильным, а более низкий уровень алкоголя делает вино более легким. Алкоголь выражен в % vol в наборе данных.

- Плотность

Плотность – это сравнение веса определенного объема вина с эквивалентным объемом воды. Он обычно используется как мера превращения сахара в спирт. В наборе данных плотность выражается в г/см3.

Другие важные аспекты атрибутов, на которые стоит обратить внимание:

- Исследуемый набор данных не содержит информации о сортах винограда, марке вина или цене.
- Классы упорядочены и не сбалансированы (например, нормальных вин гораздо больше, чем отличных или плохих).

# EDA
"""

data.info()

"""Видим значительный дисбаланс классов."""

# проверим данные на пропуски
def missing_value(df): 
    total_missing_values = df.isnull().sum() 
    missing_values_per = df.isnull().sum()/df.isnull().count() 
    null_values = pd.concat([total_missing_values, missing_values_per], axis=1, keys=['total_null', 'total_null_perc']) 
    null_values = null_values.sort_values('total_null', ascending=False) 
    return null_values[null_values['total_null'] > 0] 

missing_value(data)

"""Все пропущенные значения представляют собой очень низкий процент в числовых характеристиках. Мы заполним нулевые значения средним значением для каждого атрибута. Понимание полноты данных в наборе было простым с помощью метода .info()."""

fill_list = (missing_value(data)).index

for col in fill_list:
    data.loc[:, col].fillna(data.loc[:, col].mean(), inplace=True)

missing_value(data)

"""Пропущенные значения в нашем датасете удалены."""

data.describe().T

"""# OUTLIERS"""

data.columns

continious = ['fixed acidity', 'volatile acidity', 'citric acid',
       'residual sugar', 'chlorides', 'free sulfur dioxide',
       'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']

sns.set(style="darkgrid")
def plot_boxplots(df, column_names, nrows, ncols):
    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12,12))
    for i, column in enumerate(column_names):
        row, col = i//ncols, i%ncols
        sns.boxplot(data=df[column], ax=axes[row][col], color='g')
        axes[row][col].set_title(column)

plot_boxplots(data[continious], data[continious], 4, 3)
plt.show()

"""В нашем датасете присутсвуют выбросы."""

# удалим выбросы
def outlier_removal(data , i):
    q1 = np.quantile(data[i] ,0.25)
    q3 = np.quantile(data[i] , 0.75)
    iqr = q3-q1
    lower_tail = q1 - 1.5*iqr
    upper_tail = q3 + 1.5*iqr
    data.drop(data[data[i]> upper_tail].index , inplace = True)
    data.drop(data[data[i]< lower_tail].index ,inplace =True)
    
for  i in data[continious]:
    outlier_removal(data ,i)

print('Количество строк и столбцов после удаления выбросов:', data.shape)

"""# CORRELATION MATRIX"""

# закодируем с помощью LabelEncoder все категориальные перемнные для дальнейшего обучения модели
le = LabelEncoder()

le_count = 0
for col in data.columns:
    if data[col].dtype == 'object':
            le.fit(data[col])
            data[col] = le.transform(data[col])
            le_count += 1
print('{} columns were label encoded.'.format(le_count))

fig, ax = plt.subplots(figsize=(15, 12))
corr = data.corr()
cmap = sns.diverging_palette(220, 10, as_cmap=True)
mask = np.triu(np.ones_like(corr, dtype=np.bool))
sns.heatmap(corr, annot = True, linewidth=.8, cmap=cmap, mask=mask)

feat_corr = data.drop(columns='quality').select_dtypes('number').apply(lambda x: x.corr(data.quality))
feat_corr = pd.DataFrame(feat_corr, columns=['correlation']).sort_values(['correlation'], ascending=False)

# saving correlation heatmap
plt.figure(figsize=(10,8))
sns.barplot(x=feat_corr['correlation'], y=feat_corr.index, palette="vlag").set(
title="Feature Correlation of Numeric Features", xlabel="Feature Correlation",
ylabel="Feature Names")
plt.show()

"""Основными предикторами для целевой переменной являются:

- алкоголь
- в меньшей степени pH баланс
- плотность имеет сильную отрицательную корреляцию

type — единственная категориальная переменная с возможными значениями: «белый» или «красный».
"""

import matplotlib.pyplot as plt
import seaborn as sns

exit_counts = data['type'].value_counts()

sns.set_style('whitegrid')

plt.figure(figsize=(7,5))
ax = sns.barplot(x=exit_counts.index, y=exit_counts.values, palette='pastel')
ax.set(xlabel='Типы вин', ylabel='Количество', title='Распределение вин по типу')
plt.xticks([0, 1], ['Белое', 'Красное'])
plt.ylim(top=max(exit_counts.values)*1.1)  


plt.tight_layout() 
plt.show()

exit_percentages = round(exit_counts/len(data)*100, 1)  

print(exit_percentages)

"""Видим, что у нас сильное различие в количестве красного и белого вина. Данные о красном вине составляют всего 25% от полного набора данных, поэтому нам нужно это учитывать."""

data.columns

def distributions(df):
    fig, (ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11) = plt.subplots(11,1, figsize=(15,15))
    sns.distplot(data["fixed acidity"], ax=ax1)
    sns.distplot(data["volatile acidity"], ax=ax2)
    sns.distplot(data["citric acid"], ax=ax3)
    sns.distplot(data["residual sugar"], ax=ax4)
    sns.distplot(data["chlorides"], ax=ax5)
    sns.distplot(data["free sulfur dioxide"], ax=ax6)
    sns.distplot(data["total sulfur dioxide"], ax=ax7)
    sns.distplot(data["density"], ax=ax8)
    sns.distplot(data["pH"], ax=ax9)
    sns.distplot(data["sulphates"], ax=ax10)
    sns.distplot(data["alcohol"], ax=ax11)

    plt.tight_layout()

distributions(data[continious])

"""Данный графики показывают распределение признаков по частотности.

# PYCARET
"""

pip install pycaret

from pycaret.classification import *

# Commented out IPython magic to ensure Python compatibility.
# %time 
setup(data = data, 
      target = 'quality')

data.shape

compare_models(sort='Accuracy',n_select = 5)

(data.corr()['quality']*100).sort_values(ascending=False)

from statsmodels.stats.outliers_influence import variance_inflation_factor

vif = pd.DataFrame()
vif['VIF'] = [variance_inflation_factor(data.values, i) for i in range(data.shape[1])]
vif['variable'] = data.columns

vif.sort_values(by='VIF', ascending=False)

# encode the target variable
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

le.fit(data['quality'])

Y = pd.Series(data = le.transform(data['quality']))

X = data.drop(columns=['quality', 'type'])

X.shape

"""# BASELINE MODEL"""

from imblearn.over_sampling import SMOTE
oversampler = SMOTE(k_neighbors=4)
X,Y=oversampler.fit_resample(X,Y)

Y.value_counts()

classifiers = [
    ('Random Forest', RandomForestClassifier(n_estimators=150, random_state=42)),
    ('ET', ExtraTreesClassifier(random_state=42)),
    ('Gradient Boosting', GradientBoostingClassifier(n_estimators=150, learning_rate=0.1, max_depth=3, random_state=42)),
    ('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)),
    ('lightgbm', LGBMClassifier(random_state=42))
]



for name, classifier in classifiers:
    clf = classifier
    # Perform cross-validation with 10-folds
    cv_scores = cross_val_score(clf, X, Y, cv=10, scoring='accuracy') 
    # Print the cross-validation scores
    print(f"Average cross-validation score for {name}:", np.mean(cv_scores))
    print(f"Std cross-validation score for {name}:", np.std(cv_scores))
    print("-"*20)

"""# OPTUNA"""

!pip install optuna

import optuna

def objective(trial):
    random_state = 42
    n_jobs = -1
    max_depth = trial.suggest_int("max_depth", 80, 120)
    n_estimators = trial.suggest_int("n_estimators", 80, 120)
    min_samples_split = trial.suggest_int("min_samples_split", 2, 5)
    min_samples_leaf = trial.suggest_int("min_samples_leaf", 1, 5)
    
    classifier = ExtraTreesClassifier(random_state = random_state,
                                                  n_jobs            = n_jobs,
                                                  max_depth         = max_depth,
                                                  n_estimators      = n_estimators,
                                                  min_samples_split = min_samples_split,
                                                  min_samples_leaf  = min_samples_leaf)
    
    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
    score = cross_val_score(classifier, X, Y, cv=cv, scoring="accuracy")
    
    mean_score = score.mean()
    
    return mean_score

study = optuna.create_study(direction='maximize')
study.optimize(objective, timeout=50)

print(f"Самый высокий показатель accuracy, достигнутый в этом исследовании: {(study.best_value) * 100}%.")

study.best_params

model = ExtraTreesClassifier(max_depth=study.best_params['max_depth'], 
                               n_estimators=study.best_params['n_estimators'], 
                               min_samples_split=study.best_params['min_samples_split'], 
                               min_samples_leaf=study.best_params['min_samples_leaf'], 
                               n_jobs=2)

model.fit(X, Y)

!pip install joblib

import joblib

joblib_file = "model.pkl" 
joblib.dump(model, joblib_file)
model = joblib.load("model.pkl")